{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gemma 7B Local Playground (Apple Silicon)\n",
    "\n",
    "This notebook runs Gemma 7B locally using `llama-cpp-python` (Metal backend).\n",
    "\n",
    "If you have not installed dependencies yet in your `llm-local` conda env:\n",
    "\n",
    "```bash\n",
    "pip install -U -r requirements.txt jupyter ipykernel\n",
    "python -m ipykernel install --user --name llm-local --display-name \"Python (llm-local)\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b38355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07720de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "REPO_ID = \"bartowski/gemma-2-9b-it-GGUF\"\n",
    "FILENAME = \"gemma-2-9b-it-Q4_K_M.gguf\"\n",
    "MODEL_DIR = Path(\"./models\")\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")  # Optional, required if access-gated\n",
    "\n",
    "N_CTX = 4096\n",
    "N_BATCH = 512\n",
    "N_GPU_LAYERS = -1  # -1 = all layers on Metal\n",
    "N_THREADS = max(1, (os.cpu_count() or 8) - 2)\n",
    "MAX_TOKENS_DEFAULT = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2509f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad787cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model if needed\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=REPO_ID,\n",
    "    filename=FILENAME,\n",
    "    local_dir=MODEL_DIR,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "print(f\"Model path: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=N_CTX,\n",
    "    n_batch=N_BATCH,\n",
    "    n_gpu_layers=N_GPU_LAYERS,\n",
    "    n_threads=N_THREADS,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Warmup call\n",
    "_ = llm(\"Warmup.\", max_tokens=8, temperature=0.0)\n",
    "print(\"Model loaded and warmed up.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(prompt: str, max_tokens: int = MAX_TOKENS_DEFAULT, temperature: float = 0.2, top_p: float = 0.9):\n",
    "    \"\"\"Run a single prompt and print response + basic speed metrics.\"\"\"\n",
    "    prompt_tokens = len(llm.tokenize(prompt.encode(\"utf-8\"), add_bos=True))\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    first_token_t = None\n",
    "    out = []\n",
    "    completion_tokens = 0\n",
    "\n",
    "    stream = llm(\n",
    "        prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    for event in stream:\n",
    "        tok = event.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "        if tok:\n",
    "            out.append(tok)\n",
    "            completion_tokens += 1\n",
    "            if first_token_t is None:\n",
    "                first_token_t = time.perf_counter()\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    total_s = end - start\n",
    "    ttft_s = (first_token_t - start) if first_token_t is not None else total_s\n",
    "    decode_s = (end - first_token_t) if first_token_t is not None else 0.0\n",
    "\n",
    "    decode_tps = completion_tokens / decode_s if decode_s > 0 else 0.0\n",
    "    e2e_tps = completion_tokens / total_s if total_s > 0 else 0.0\n",
    "\n",
    "    text = \"\".join(out).strip()\n",
    "\n",
    "    print(\"Prompt:\")\n",
    "    print(prompt)\n",
    "    print(\"\\nResponse:\")\n",
    "    print(text)\n",
    "    print(\"\\nMetrics:\")\n",
    "    print(f\"prompt_tokens: {prompt_tokens}\")\n",
    "    print(f\"completion_tokens: {completion_tokens}\")\n",
    "    print(f\"ttft_s: {ttft_s:.3f}\")\n",
    "    print(f\"decode_tps: {decode_tps:.2f}\")\n",
    "    print(f\"end_to_end_tps: {e2e_tps:.2f}\")\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"response\": text,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"completion_tokens\": completion_tokens,\n",
    "        \"ttft_s\": ttft_s,\n",
    "        \"decode_tps\": decode_tps,\n",
    "        \"end_to_end_tps\": e2e_tps,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73880dc1",
   "metadata": {},
   "source": [
    "### Throughput benchmark\n",
    "\n",
    "The helper below measures how many completion tokens per second the model produces for a given prompt.  It reports both end-to-end and decode-only rates, which are useful when comparing configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7d9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_throughput(prompt: str, max_tokens: int = MAX_TOKENS_DEFAULT,\n",
    "                       temperature: float = 0.2, top_p: float = 0.9):\n",
    "    \"\"\"Send a prompt and return tokens‑per‑second stats.\"\"\"\n",
    "    start = time.perf_counter()\n",
    "    first_token = None\n",
    "    out_tokens = 0\n",
    "\n",
    "    stream = llm(prompt,\n",
    "                 max_tokens=max_tokens,\n",
    "                 temperature=temperature,\n",
    "                 top_p=top_p,\n",
    "                 stream=True)\n",
    "\n",
    "    for evt in stream:\n",
    "        tok = evt.get(\"choices\", [{}])[0].get(\"text\", \"\")\n",
    "        if tok:\n",
    "            out_tokens += 1\n",
    "            if first_token is None:\n",
    "                first_token = time.perf_counter()\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    total = end - start\n",
    "    decode = (end - first_token) if first_token is not None else total\n",
    "\n",
    "    return {\n",
    "        \"prompt_len\": len(prompt),\n",
    "        \"completion_tokens\": out_tokens,\n",
    "        \"total_s\": total,\n",
    "        \"decode_s\": decode,\n",
    "        \"e2e_tps\": out_tokens / total if total > 0 else float(\"inf\"),\n",
    "        \"decode_tps\": out_tokens / decode if decode > 0 else float(\"inf\"),\n",
    "    }\n",
    "\n",
    "# example usage with nicer formatting\n",
    "prompt = \"Repeat after me: the quick brown fox jumps over the lazy dog. \" * 50\n",
    "stats = measure_throughput(prompt, max_tokens=500)\n",
    "\n",
    "print(\"=== throughput benchmark ===\")\n",
    "print(f\"prompt length (chars): {stats['prompt_len']}\")\n",
    "print(f\"completion tokens: {stats['completion_tokens']}\")\n",
    "print(f\"total elapsed: {stats['total_s']:.3f}s\")\n",
    "print(f\"decode phase: {stats['decode_s']:.3f}s\")\n",
    "print(f\"end-to-end throughput: {stats['e2e_tps']:.2f} tokens/s\")\n",
    "print(f\"decode-only throughput: {stats['decode_tps']:.2f} tokens/s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try prompts\n",
    "result_1 = run_prompt(\"Explain transformers to a 12-year-old in 5 bullet points.\")\n",
    "\n",
    "# Add your own:\n",
    "result_2 = run_prompt(\"Write a Python function for quicksort with docstring.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00412b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm-local)",
   "language": "python",
   "name": "llm-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
